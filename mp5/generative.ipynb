{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a87951e731cc9fe5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# CS 444: Deep Learning for Computer Vision, Fall 2024, Assignment 5\n",
    "\n",
    "Assignment is due at 11:59:59 PM on Thursday Dec 5 2024.\n",
    "\n",
    "1. Assignment is due at **11:59:59 PM on Thursday Dec 5 2024**.\n",
    "\n",
    "2. See [policies](https://saurabhg.web.illinois.edu/teaching/cs444/fa2024/policies.html) on [class website](https://saurabhg.web.illinois.edu/teaching/cs444/fa2024).\n",
    "\n",
    "3. Submission instructions:\n",
    "    1. On gradescope assignment called `MP5-code`, upload the following  files:\n",
    "        - Your completed `vae.py`, `score.py`, `networks.py`, and `train_mnist.py` files.\n",
    "\n",
    "       Please also note the following points:\n",
    "        - Do not compress the files into `.zip` as this will not work.\n",
    "        - Do not change the provided files names nor the names of the functions but rather change the code inside the provided functions and add new functions. Also, make sure that the inputs and outputs of the provided functions are not changed.\n",
    "        - The autograder will give you feedback on how well your code did for the autograded parts.\n",
    "        - The autograder is configured with the python libraries: `numpy matplotlib tqdm torch` only.\n",
    "\n",
    "4. Lastly, be careful not to work of a public fork of this repo. Make a private clone to work on your assignment. You are responsible for preventing other students from copying your work.\n",
    "\n",
    "In this notebook, we will implement the VAEs and Score Matching ([Noise-Conditioned Score Network](https://arxiv.org/abs/1907.05600)) models and compare their performance on the swiss roll dataset. We will also use NCSNs to generate MNIST images.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. To start, you will implement the network architectures defined in `vae.py` and `score.py`. You can use this Jupyter Notebook for testing your implementation and visualizing your results. You don't need a serious GPU for Q1 and Q2.\n",
    "    1. To facilitate your testing, enable the autoreload extension so that your modifications to the `.py` files will be automatically reloaded. In our testing, we found this to be somewhat finicky and had to restart the kernel for it to pickup the changed files for some parts.\n",
    "    \n",
    "2. One you successfully finish the score-matching parts, you are ready to tackle Q3 where you will be training a NCSN network to generate MNIST digits. This may need a good GPU and you are welcome to use the CampusCluster or Google Colab or Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f4838c06f620b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:44:06.073902500Z",
     "start_time": "2023-11-09T04:44:06.018979900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5422e2537c8460ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:44:10.292061400Z",
     "start_time": "2023-11-09T04:44:06.992686800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import unittest\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vae import VAE\n",
    "from score import ScoreNet\n",
    "from networks import SimpleDecoder, SimpleEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c88828c7c0309c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Swiss Roll Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7bc5138d68336a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:44:10.405545400Z",
     "start_time": "2023-11-09T04:44:10.294001800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def make_swiss_roll(n_samples=100, *, noise=0.0, seed=None):\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(seed)\n",
    "    t = 1.5 * np.pi * (1 + 2 * torch.rand(n_samples, generator=generator))\n",
    "    x = t * torch.cos(t)\n",
    "    y = t * torch.sin(t)\n",
    "\n",
    "    X = torch.stack((x, y), dim=1)\n",
    "    X += noise * torch.randn(n_samples, 2, generator=generator)\n",
    "    return X, t\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1161402f76e367f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X, t = make_swiss_roll(n_samples=1000, noise=1., seed=42)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=t, cmap=plt.cm.hot)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d51e2cce3b2e2ad",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Q1. VAE (Swiss Roll)\n",
    "\n",
    "**Q1.1 [1 pts Autograded]** Implement the `reparameterize` method in `vae.py`. This method takes in the mean and log-variance of the latent distribution and returns a sample from the distribution.\n",
    "\n",
    "Once you have implemented the method, run the following cell to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d150ef2b505b4e9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "suite = unittest.TestLoader().loadTestsFromName('test_models.TestModels.test_vae_reparam')\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cb0921fc39904d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Q1.2 [3 pts Autograded]** Implement the `get_loss` method in `vae.py`. This method takes in a batch of data and returns the VAE loss. You should compute the reconstruction loss and the KL divergence loss and return the sum of the two losses. The reconstruction loss is the **mean squared error** between the reconstructed images and the original images. The KL divergence loss can be computed in closed form, and you should **sum** it along latent dimensions.\n",
    "\n",
    "**Hint**. The KL-divergence between a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$ and a standard Gaussian distribution $\\mathcal{N}(0,1)$ is given by:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_{\\text{KL}}(\\mathcal{N}(\\mu, \\sigma^2) || \\mathcal{N}(0, 1)) = \\frac{1}{2}\\left( \\sigma^2 + \\mu^2 - 1 - \\log \\sigma^2 \\right)\n",
    "$$\n",
    "\n",
    "Once you have implemented the method, run the following cell to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6019c9f75a078",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "suite = unittest.TestLoader().loadTestsFromName('test_models.TestModels.test_vae_loss')\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21796606d7947e0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now train your VAE model on the Swiss roll dataset and try to generate some points after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9564fdb2ad0e423",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vae = VAE(\n",
    "    SimpleEncoder(input_size=2, hidden_size=128, latent_size=10 * 2),\n",
    "    SimpleDecoder(latent_size=10, hidden_size=128, output_size=2)\n",
    ")\n",
    "print(f'Number of parameters in VAE: {count_parameters(vae)}')\n",
    "vae.train()\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "for epoch in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    loss = vae.get_loss(X)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 200 == 0:\n",
    "        print(f'Epoch {epoch}, loss {loss.item():.4f}')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa43de4eddba49c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_sample = 1000\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    X_gen = vae.sample(n_sample, 'cpu')\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=t, cmap=plt.cm.hot)\n",
    "plt.scatter(X_gen[:, 0], X_gen[:, 1], c='b', marker='x')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c6b322a0af16a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Q2: Score Matching (Swiss Roll)\n",
    "\n",
    "In this part, you are expected to implement the score matching model on the Swiss roll dataset.\n",
    "\n",
    "**Q2.1 [2 pts Autograded]** Implement the `perturb` method in `score.py`. This function takes in a batch of data and returns a perturbed batch of data. Specifically, you should first choose uniformly random from `self.simgas` for each data (NOT each pixel/coordinate!). Then add the Gaussian noise the data with standard deviation `sigma`. Return the applied noise and used sigma values. DO NOT clip the perturbed data!\n",
    "\n",
    "Once you have implemented the method, run the following cell to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16606041747802a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "suite = unittest.TestLoader().loadTestsFromName('test_models.TestModels.test_score_perturb')\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12426c7d224242a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Q2.2 [3 pts Autograded]** Implement the `get_loss` method in `score.py`. You should first call `perturb` to obtain the noise and perturbed data and call `get_score` to obtain the estimated scores for the perturbed data. The score loss in NCSNv2 is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{score}} = \\frac{1}{2}\\mathbb{E}_{\\sigma} \\mathbb{E}_{\\hat{\\mathbf{x}}\\sim p(\\mathbf{\\hat{x}},\\mathbf{x})} \\left[ \\left( s(\\hat{\\mathbf{x}}) - \\frac{\\mathbf{x} - \\mathbf{\\hat{x}}}{\\sigma^2} \\right)^2\\cdot \\sigma^2 \\right]\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}$ is the original data and $\\mathbf{\\hat{x}}$ is the perturbed data. $\\sigma$ is the standard deviation of the noise, and $s(\\hat{\\mathbf{x}})$ is the estimated score for the perturbed data. The loss should be **averaged** over the batch dimension and the image dimensions.\n",
    "\n",
    "Once you have implemented the method, run the following cell to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b64e9c3d8e5988",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "suite = unittest.TestLoader().loadTestsFromName('test_models.TestModels.test_score_loss')\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be74da5cf394b201",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Q2.3 [2 pts Autograded]** Complete the `sample` method in `score.py`. Recall that in Langevin dynamics, the update rule is:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{t+1} = \\mathbf{x}_t + \\eta\\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}) + \\sqrt{2\\eta}\\mathbf{z}_t\n",
    "$$\n",
    "\n",
    "where $\\mathbf{z}_t\\sim \\mathcal{N}(0, \\mathbf{I})$. In this method, we first sample a batch of noise $\\mathbf{z}_t$ and estimate the scores (gradient of the log density $\\nabla_{\\mathbf{x}}\\log p(\\mathbf{x})$) using `get_score`. Then you should compute the update $\\mathbf{x}_{t+1}$ and return the sequence of updates.\n",
    "\n",
    "Once you have implemented the method, run the following cell to test your implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1311b2027b026e19",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "suite = unittest.TestLoader().loadTestsFromName('test_models.TestModels.test_score_sample')\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c899e8f932a6939f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "scorenet = ScoreNet(\n",
    "    torch.nn.Sequential(\n",
    "        SimpleEncoder(input_size=2, hidden_size=128, latent_size=16),\n",
    "        SimpleDecoder(latent_size=16, hidden_size=128, output_size=2),\n",
    "    ), 10., 0.1, 20, 'geometric'\n",
    ")\n",
    "scorenet.train()\n",
    "optimizer = optim.Adam(scorenet.parameters(), lr=1e-3)\n",
    "for epoch in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    loss = scorenet.get_loss(X)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 200 == 0:\n",
    "        print(f'Epoch {epoch}, loss {loss.item():.4f}')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8d13bea15290a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_sample = 1000\n",
    "scorenet.eval()\n",
    "with torch.no_grad():\n",
    "    X_gen = scorenet.sample(n_sample, 2, step_lr=2e-3)[-1, -1]\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=t, cmap=plt.cm.hot)\n",
    "plt.scatter(X_gen[:, 0], X_gen[:, 1], c='b', marker='x')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e903f7b6ea526a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Q2.4 [1 pts Manual graded]** Compare the sampled points from VAE and score matching and upload the visualization. Which model do you think performs the best?\n",
    "\n",
    "**Q2.5 [1 pts Manual graded]** Modify the `step_lr` argument in the score-matching sampling and rerun the sampling for at least three different step sizes. Upload the visualization and discuss the effect of the step size on the sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31a5730-42ec-4416-b1f0-a8ab09488c8f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Q3: Score Matching (MNIST)\n",
    "\n",
    "**Q3 [3 pts Manual graded]** Next, we will use Score Matching on the MNIST dataset to generate MNIST digits. An example implementation that only uses the SimpleEncoder and SimpleDecoder networks is provided in `train_mnist.py`. As provided, the network doesn't work and generated samples are of bad quality. Your task is to make it work such that it starts generating good samples. You might have to adjust the hyper-parameters and experiment with different networks. We found success with a UNet architecture. Note that the MNIST dataset here contains 32 x 32 images instead of the usual 28 x 28 images. This may simplify implementation of a UNet. Our implementation is able to generate reasonably looking samples in a few minutes of training on a 2080Ti GPU.\n",
    "\n",
    "You can use following command to get started, though as noted, this doesn't produce good samples. It will store tensorboard logs (with samples) and also save generated samples as images.\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=1 python train_mnist.py --num_epochs 30 --model_type simple_fc --output_dir runs/fc-v1\n",
    "```\n",
    "\n",
    "You can also use the following command to launch the job on campuscluster:\n",
    "```bash\n",
    "sbatch --export=ALL,OUTPUT_DIR=\"runs/fc-v1/\" --output=\"runs/fc-v1/%j.out\" --error=\"runs/fc-v1/%j.err\" sample.sbatch\n",
    "```\n",
    "\n",
    "Upload the samples generated by your best model, and describe the changes you had to make (architecture, hyper-parameters) to get these samples. Feel free to include samples from intermediate runs if useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa83265c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
